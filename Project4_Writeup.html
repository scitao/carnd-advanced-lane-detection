<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=yrPxF2fzIbKO47OcVFfZ2CvhBULzVSj67-CyYQmQBzclLT34rKZNjX_x5wwcqB1FpQwIf4b_mAvI61pP2Pbpm44s2tWezz3jm8Y284-wViVYgHWNks-YO-d3TrgekytC');ul.lst-kix_jdvyd4zeu269-4{list-style-type:none}ul.lst-kix_jdvyd4zeu269-3{list-style-type:none}ul.lst-kix_jdvyd4zeu269-2{list-style-type:none}ul.lst-kix_jdvyd4zeu269-1{list-style-type:none}ul.lst-kix_jdvyd4zeu269-8{list-style-type:none}ul.lst-kix_jdvyd4zeu269-7{list-style-type:none}ul.lst-kix_jdvyd4zeu269-6{list-style-type:none}ul.lst-kix_jdvyd4zeu269-5{list-style-type:none}ul.lst-kix_jdvyd4zeu269-0{list-style-type:none}.lst-kix_28d4nubxhxft-0>li:before{content:"\0025cf  "}.lst-kix_28d4nubxhxft-1>li:before{content:"\0025cb  "}.lst-kix_28d4nubxhxft-3>li:before{content:"\0025cf  "}.lst-kix_28d4nubxhxft-2>li:before{content:"\0025a0  "}.lst-kix_28d4nubxhxft-5>li:before{content:"\0025a0  "}.lst-kix_28d4nubxhxft-4>li:before{content:"\0025cb  "}.lst-kix_jdvyd4zeu269-3>li:before{content:"\0025cf  "}.lst-kix_jdvyd4zeu269-2>li:before{content:"\0025a0  "}.lst-kix_jdvyd4zeu269-4>li:before{content:"\0025cb  "}.lst-kix_28d4nubxhxft-6>li:before{content:"\0025cf  "}.lst-kix_jdvyd4zeu269-0>li:before{content:"\0025cf  "}.lst-kix_jdvyd4zeu269-8>li:before{content:"\0025a0  "}.lst-kix_28d4nubxhxft-7>li:before{content:"\0025cb  "}.lst-kix_jdvyd4zeu269-1>li:before{content:"\0025cb  "}.lst-kix_28d4nubxhxft-8>li:before{content:"\0025a0  "}ul.lst-kix_28d4nubxhxft-8{list-style-type:none}ul.lst-kix_28d4nubxhxft-3{list-style-type:none}.lst-kix_jdvyd4zeu269-7>li:before{content:"\0025cb  "}ul.lst-kix_28d4nubxhxft-2{list-style-type:none}ul.lst-kix_28d4nubxhxft-1{list-style-type:none}.lst-kix_jdvyd4zeu269-6>li:before{content:"\0025cf  "}ul.lst-kix_28d4nubxhxft-0{list-style-type:none}ul.lst-kix_28d4nubxhxft-7{list-style-type:none}.lst-kix_jdvyd4zeu269-5>li:before{content:"\0025a0  "}ul.lst-kix_28d4nubxhxft-6{list-style-type:none}ul.lst-kix_28d4nubxhxft-5{list-style-type:none}ul.lst-kix_28d4nubxhxft-4{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c2{margin-left:36pt;padding-top:10pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c4{margin-left:-0.8pt;padding-top:24pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left;margin-right:89.2pt}.c14{background-color:#00e2e2;color:#ffffff;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Economica";font-style:normal}.c8{margin-left:-0.8pt;padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c33{margin-left:-0.8pt;padding-top:30pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c15{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Trebuchet MS";font-style:italic}.c11{margin-left:3.8pt;padding-top:10pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Roboto";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c22{margin-left:3.8pt;padding-top:6pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:center}.c16{padding-top:0pt;text-indent:0.8pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c18{margin-left:-0.8pt;padding-top:10pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c10{margin-left:-0.8pt;padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c12{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Open Sans";font-style:normal}.c7{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Open Sans";font-style:normal}.c23{margin-left:-0.8pt;padding-top:3pt;padding-bottom:0pt;line-height:1.5;orphans:2;widows:2;text-align:left}.c21{color:#000000;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Open Sans";font-style:normal}.c26{color:#999999;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Economica";font-style:normal}.c6{padding-top:8pt;padding-bottom:0pt;line-height:1.15;page-break-after:avoid;text-align:left}.c32{text-decoration:none;vertical-align:baseline;font-family:"Economica";font-style:normal}.c5{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{color:inherit;text-decoration:inherit}.c27{padding:0;margin:0}.c9{font-family:"Roboto Mono";font-weight:400}.c31{color:#1155cc;text-decoration:underline}.c30{font-size:30pt;color:#000000}.c28{height:16pt}.c1{background-color:#ffe599}.c29{font-weight:400}.c25{background-color:#a4c2f4}.c17{height:11pt}.c19{height:13pt}.c24{background-color:#93c47d}.c13{font-weight:700}.title{padding-top:0pt;color:#000000;font-size:30pt;padding-bottom:0pt;font-family:"Economica";line-height:1.0;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#999999;font-size:14pt;padding-bottom:0pt;font-family:"Economica";line-height:1.0;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Open Sans"}p{margin:0;color:#000000;font-size:11pt;font-family:"Open Sans"}h1{padding-top:10pt;color:#000000;font-weight:700;font-size:16pt;padding-bottom:0pt;font-family:"Open Sans";line-height:1.5;orphans:2;widows:2;text-align:left}h2{padding-top:24pt;color:#000000;font-weight:700;font-size:13pt;padding-bottom:0pt;font-family:"Open Sans";line-height:1.0;orphans:2;widows:2;text-align:left}h3{padding-top:10pt;color:#8c7252;font-weight:700;font-size:12pt;padding-bottom:0pt;font-family:"Open Sans";line-height:1.5;orphans:2;widows:2;text-align:left}h4{padding-top:8pt;color:#666666;text-decoration:underline;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.5;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.5;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c5"><div><p class="c17 c33 subtitle" id="h.leajue2ys1lr"><span class="c26 c29"></span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 2.67px;"><img alt="" src="images/image05.png" style="width: 624.00px; height: 2.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="horizontal line"></span></p></div><p class="c10"><span class="c14">&nbsp;UDACITY SELF DRIVING CAR ENGINEER NANODEGREE &nbsp;</span></p><p class="c18"><span class="c20">Term 1: Computer Vision and Deep Learning</span></p><p class="c16 title" id="h.tn9v0bw7tbt0"><span>PROJECT 4</span></p><p class="c10 subtitle" id="h.vb8p0lepu9vn"><span class="c13 c30">ADVANCED LANE FINDING </span></p><p class="c23"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 4.00px;"><img alt="" src="images/image04.png" style="width: 624.00px; height: 4.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="horizontal line"></span></p><h1 class="c22" id="h.vydniszftb1n"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 544.50px; height: 627.97px;"><img alt="" src="images/image13.png" style="width: 544.50px; height: 627.97px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><h1 class="c11" id="h.mgaxo5sypbmk"><span class="c21 c13">Introduction</span></h1><p class="c8"><span class="c0">The goals / steps of this project are the following: </span></p><ul class="c27 lst-kix_jdvyd4zeu269-0 start"><li class="c2"><span class="c0">Compute the camera calibration matrix and distortion coef&#64257;cients given a set of chessboard images. </span></li><li class="c2"><span class="c0">Apply a distortion correction to raw images. </span></li><li class="c2"><span class="c0">Use color transforms, gradients, etc., to create a thresholded binary image. </span></li><li class="c2"><span class="c0">Apply a perspective transform to rectify binary image (&quot;bird&#39;s-eye view&quot;). </span></li><li class="c2"><span class="c0">Detect lane pixels and &#64257;t to &#64257;nd the lane boundary. </span></li><li class="c2"><span class="c0">Determine the curvature of the lane with respect to center. </span></li><li class="c2"><span class="c0">Warp the detected lane boundaries back onto the original image. </span></li><li class="c2"><span class="c0">Output visual display of the lane boundaries and numerical estimation of lane curvature.</span></li></ul><p class="c8"><span class="c0">Project submission includes following files:</span></p><ul class="c27 lst-kix_28d4nubxhxft-0 start"><li class="c2"><span class="c0">Writeup &lsquo;project4_writeup.html&rsquo; file (you&rsquo;re reading it right now).</span></li><li class="c2"><span class="c0">Jupyter notebook &lsquo;advanced_lane_detection.ipynb&rsquo;, which contains all project code and additional commentary of the project implementation.</span></li><li class="c2"><span class="c0">Example output images for each stage of the processing pipeline in the &lsquo;output_images&rsquo; folder.</span></li><li class="c2"><span class="c0">Output video files in the &lsquo;output_videos&rsquo; folder.</span></li></ul><p class="c8"><span class="c0">This writeup includes statements and supporting figures / images that explain how each rubric item was addressed, and specifically where in the code each step was handled. Also, please take a look at jupyter notebook, which contains additional commentary for pipeline implementation.</span></p><p class="c8"><span>All rubric points addressed in order and described accordingly to </span><span class="c31"><a class="c3" href="https://www.google.com/url?q=https://review.udacity.com/%23!/rubrics/571/view&amp;sa=D&amp;ust=1488096805078000&amp;usg=AFQjCNGZXtxD2pCUix3WxZhmJIlq87zu_g">Project Specifications</a></span></p><h1 class="c11 c28" id="h.n5lsyqftlsyt"><span class="c21 c13"></span></h1><h1 class="c11" id="h.pw3p9u5881g2"><span class="c13 c21">Rubric points</span></h1><h2 class="c4" id="h.sxxy9dka0now"><span class="c12">Camera Calibration</span></h2><h6 class="c6" id="h.v4i8rleqfjru"><span class="c15">CRITERIA: Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.</span></h6><p class="c8"><span>T</span><span class="c0">he code for this step is contained in the &#64257;rst and second code cells of the IPython notebook.</span></p><p class="c8"><span>For each camera calibration image I am computing chessboard corners position using </span><span class="c1 c9">cv2.findChessboardCorners</span><span>&nbsp;function applied to the grayscaled original image. All found object and corner points stored in separate arrays which are then used as arguments for the </span><span class="c1">cv2.calibrateCamera</span><span>&nbsp;function which produces calibration and distortion coefficients of the camera. Using these coefficients I am undistorting test calibration images using </span><span class="c1">cv2.undistort</span><span>&nbsp;function, examples of output are in the </span><span class="c13">&lsquo;output_images/out_calibration[N].jpg&rsquo;</span><span class="c0">&nbsp;files. </span></p><p class="c8"><span class="c0">&nbsp;Here is an example of output:</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 172.00px;"><img alt="" src="images/image08.png" style="width: 624.00px; height: 172.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c17"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c8 c17"><span class="c0"></span></p><h2 class="c4" id="h.y0u9ekmvs519"><span class="c12">Pipeline (test images)</span></h2><p class="c8 c17"><span class="c0"></span></p><h6 class="c6" id="h.qei02qew56bm"><span class="c15">CRITERIA: Provide an example of a distortion-corrected image.</span></h6><p class="c8"><span class="c0">Code for this step contained it the # 3 code cell of the IPython notebook</span></p><p class="c8"><span>I am applying distortion and calibration coefficients for the camera obtained in the previous step &nbsp;to test road images, using </span><span class="c1">cv2.undistort</span><span>&nbsp;function. Output files are in the </span><span class="c13">&lsquo;output_images/calibrated_[image_name].jpg&rsquo;</span><span class="c0">&nbsp;files. &nbsp;</span></p><p class="c8"><span class="c0">Here is an example of the undistorted test image:</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 675.06px; height: 393.17px;"><img alt="" src="images/image10.png" style="width: 675.06px; height: 393.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c17"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c8 c17"><span class="c0"></span></p><h6 class="c6" id="h.k53slbgmpau"><span class="c15">CRITERIA: Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image. Provide an example of a binary image result.</span></h6><p class="c8"><span class="c0">Code for this step contained in the # 4 and &nbsp;# 5 code cells of the IPython Notebook.</span></p><p class="c8"><span>Firstly, I&rsquo;m creating a HLS color space version of the original image using </span><span class="c1">cv2.cvtColor(undist_img, cv2.COLOR_RGB2HLS)</span><span>&nbsp;and separating S channel, which contains a lot of information about possible lane lines on the image. After that I am computing absolute scaled sobel derivative to accentuate lines away from horizontal using </span><span class="c1">cv2.Sobel</span><span>&nbsp;function. Using th</span><span>is derivative</span><span>, I am computing threshold X gradient using threshold values in the range </span><span class="c13">(20, 100)</span><span>. I am doing the same thing for S color channel, thresholding it values in the range </span><span class="c13">(170, 255)</span><span>. So now I have two binary thresholds arrays, one for sobel derivative, and second for the S channel. Now I&rsquo;m stacking these &nbsp;binary arrays into one binary array to produce binary image with highlighted possible lane lines. Output files are in the </span><span class="c13">&lsquo;output_images/thresholds_[image_name].jpg&rsquo;</span><span>&nbsp;files. <br>Here is an example of the original and resulting image after thresholding have been applied (on stacked thresholds image </span><span>green color</span><span>&nbsp;stands for </span><span class="c24">Sobel derivative threshold</span><span>&nbsp;and blue for </span><span class="c25">S channel threshold</span><span class="c0">):</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 571.50px; height: 341.03px;"><img alt="" src="images/image09.png" style="width: 571.50px; height: 341.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 373.33px;"><img alt="" src="images/image12.png" style="width: 624.00px; height: 373.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 377.33px;"><img alt="" src="images/image06.png" style="width: 624.00px; height: 377.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c6" id="h.4zd0jm27pq6j"><span class="c15">CRITERIA: Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.</span></h6><p class="c8"><span class="c0">Code for this step contained in the # 6 and # 7 code cells of the IPython Notebook.</span></p><p class="c8"><span>To perform a perspective transform, I am using </span><span class="c1">cv2.getPerspectiveTransform</span><span>&nbsp;function applied to source and destination points coordinates, which produce matrix for perspective image transformation. Then I am using </span><span class="c1">cv2.warpPerspective </span><span>function to get warped image</span><span><br>Output files are in the </span><span class="c13">&lsquo;output_images/perspective_[image_name].jpg&rsquo;</span><span class="c0">&nbsp;files. </span></p><p class="c8"><span class="c0">Before applying perspective transform, I perform image thresholding. Here is an example of what result image looks like with highlighted source and destination areas:</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 380.00px;"><img alt="" src="images/image14.png" style="width: 624.00px; height: 380.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 380.00px;"><img alt="" src="images/image01.png" style="width: 624.00px; height: 380.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c17"><span class="c0"></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 373.33px;"><img alt="" src="images/image11.png" style="width: 624.00px; height: 373.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c6" id="h.ds1ycsmdlrmx"><span class="c15">CRITERIA: Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?</span></h6><p class="c8"><span class="c0">Code for this step contained in the # 8 code cell of the iPython Notebook.</span></p><p class="c8"><span>After applying calibration, thresholding, and a perspective transform to a road image, I have a binary image where the lane lines stand out clearly. To decide explicitly which pixels are part of the lines and which belong to the left line and which belong to the right line I first take a histogram along all the columns in the lower half of the image: &nbsp;<br></span><span class="c1">histogram = np.sum(binary_warped[binary_warped.shape[0]/2:,:], axis=0)</span><span><br>Then I find the peak of the left and right halves of the histogram. These will be the starting point for the left and right lines. After this I am defining sliding windows starting from these points &nbsp;and stepping through the windows one by one to identify non-zero pixels and append these indices to the line pixels lists. This allows me to extract left and right line pixels positions. Then I am fitting those positions into polynomial coefficients using </span><span class="c1">np.polyfit </span><span>function.<br></span><span>Output files are in the </span><span class="c13">&lsquo;output_images/fit_poly_[image_name].jpg&rsquo;</span><span class="c0">&nbsp;files. <br>Here is example of output:</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 372.00px;"><img alt="" src="images/image15.png" style="width: 624.00px; height: 372.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 346.67px;"><img alt="" src="images/image03.png" style="width: 624.00px; height: 346.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c8 c17"><span class="c7"></span></p><hr style="page-break-before:always;display:none;"><p class="c8 c17"><span class="c7"></span></p><h6 class="c6" id="h.eu1lx1pv5qdc"><span class="c15">CRITERIA: Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.</span></h6><p class="c8"><span class="c0">Code for this step contained in the # 11 code cells of the iPython Notebook.</span></p><p class="c8"><span>I have a polynomial coefficients for the lane lines functions. To find out a curvature of the lane I am simply using formula </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 153.00px; height: 54.00px;"><img alt="" src="images/image07.png" style="width: 153.00px; height: 54.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp;. Also, because I need to transform pixel image distance to meters, I use conversion coefficients for </span><span class="c13">y = 30/720</span><span>&nbsp;and for </span><span class="c13">x = 3.7/700 </span><span class="c0">meters per pixels in y and x dimensions respectively. <br>To find out a car position relatively to the center of the lane I use bottom points of the lane lines functions , so now I know left and right border of the lane relatively to the car. Distance from the center of the image to lane border will define relative car position to the center of the lane. You can see lane curvature radius and relative car position being displayed on the result video output of the pipeline.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Here is an example of the videostream image with the lane curvature and lane position being displayed in the top left corner:</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 645.50px; height: 363.09px;"><img alt="" src="images/image00.png" style="width: 645.50px; height: 363.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c6" id="h.45q1eyvslhwa"><span class="c15">CRITERIA: Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.</span></h6><p class="c8"><span class="c0">Code for this step contained in the # 8 code cells of the iPython Notebook.</span></p><p class="c8"><span>To project lane area back to the original image I use a backward warp function </span><span class="c1">cv2.warpPerspective</span><span>&nbsp;(with inverted coefficients) of the result lane projection. And then I use </span><span class="c1">cv2.addWeighted</span><span class="c0">&nbsp; function to highlight lane area on the original image.</span></p><p class="c8"><span>Output files are in the </span><span class="c13">&lsquo;output_images/fit_poly_[image_name].jpg&rsquo;</span><span class="c0">&nbsp;files. </span></p><p class="c8"><span class="c0">Here is an example of original test image with projected lane area:</span></p><p class="c8"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 657.69px; height: 374.17px;"><img alt="" src="images/image02.png" style="width: 657.69px; height: 374.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c4 c19" id="h.iwbwrpslt4f0"><span class="c12"></span></h2><hr style="page-break-before:always;display:none;"><h2 class="c4 c19" id="h.vqzsua2bwwav"><span class="c12"></span></h2><h2 class="c4" id="h.6lh1x26gqrin"><span class="c12">Pipeline (video)</span></h2><h6 class="c6" id="h.3k3np1ycpprw"><span class="c15">CRITERIA: Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!)</span></h6><p class="c8"><span class="c0">Processing of videos implementation code contained in the # 11 - 17 code cells of the IPython Notebook.</span></p><p class="c8"><span>Output files are in the </span><span class="c13">&lsquo;output_videos&rsquo;</span><span class="c0">&nbsp;folder. </span></p><p class="c8"><span class="c31"><a class="c3" href="./output_videos/project_video_result.mp4">Resulting video</a></span></p><h2 class="c4" id="h.hvync1ygxz8q"><span class="c12">Discussion</span></h2><h6 class="c6" id="h.x69o4vudcv49"><span class="c15">CRITERIA: Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?</span></h6><p class="c8"><span class="c0">Most time consuming issues are include fine-tuning perspective transform points coordinates (I ended up using points coordinates provided with the example writeup file for the project). Also it is worth mentioning that thresholding range values also have to be picked up manually, and it is not an ordinary task, because even small difference make a big change for the resulting lane lines image.</span></p><p class="c8"><span class="c0">Most likely my pipeline will fail on the videos with not clearly visible lane lines . As you can see, challenge videos was handled not correctly with my pipeline. I will try to improve my pipeline in meantime because overall it is a very profound and rewarding task.</span></p><p class="c8"><span class="c0">To make my pipeline more robust I need to implement more sophisticated lane lines search algorithm. Including advanced window search algorithm and implement outlier rejection and use a low-pass filter to smooth the lane detection over frames.</span></p><p class="c8 c17"><span class="c0"></span></p><p class="c8 c17"><span class="c0"></span></p></body></html>